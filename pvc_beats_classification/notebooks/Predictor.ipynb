{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "# from biosppy.signals.ecg import correct_rpeaks\n",
    "from scipy.signal import resample\n",
    "import sys\n",
    "from seaborn import scatterplot\n",
    "sys.path.append('/home/bohdan/PeakDetector')\n",
    "from modules.baseline import ResNet\n",
    "# from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "import shutil\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "# from utils import normalize\n",
    "from scipy import signal\n",
    "import glob\n",
    "\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint='checkpoint_fpn', filename='checkpoint.pth.tar', snapshot=None):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "\n",
    "    if snapshot and state.epoch % snapshot == 0:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'checkpoint_{}.pth.tar'.format(state.epoch)))\n",
    "\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv1d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class SignalNet(nn.Module):\n",
    "    def __init__(self, n_class, n_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dconv_down1 = double_conv(n_channels, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)\n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "\n",
    "        self.conv_last = nn.Conv1d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shapes = []\n",
    "\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "        x_shapes.append(x.shape)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        x_shapes.append(x.shape)\n",
    "\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)\n",
    "        x_shapes.append(x.shape)\n",
    "\n",
    "        x = self.dconv_down4(x)\n",
    "        x_shapes.append(x.shape)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x_shapes.append(x.shape)\n",
    "\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x_shapes.append(x.shape)\n",
    "\n",
    "        x = self.dconv_up3(x)\n",
    "        x_shapes.append(x.shape)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x_shapes.append(x.shape)\n",
    "\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x_shapes.append(x.shape)\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x_shapes.append(x.shape)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x_shapes.append(x.shape)\n",
    "\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        x_shapes.append(x.shape)\n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class IncResBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, convstr=1, convsize=15, convpadding=7):\n",
    "        super(IncResBlock, self).__init__()\n",
    "        self.Inputconv1x1 = nn.Conv1d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n",
    "        self.conv1_1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=inplanes,\n",
    "                out_channels=planes//4,\n",
    "                kernel_size=convsize,\n",
    "                stride=convstr,\n",
    "                padding=convpadding\n",
    "            ),\n",
    "            nn.BatchNorm1d(planes//4))\n",
    "        self.conv1_2 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                inplanes,\n",
    "                planes//4,\n",
    "                kernel_size=1,\n",
    "                stride=convstr,\n",
    "                padding=0,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm1d(planes//4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(\n",
    "                in_channels=planes//4,\n",
    "                out_channels=planes//4,\n",
    "                kernel_size=convsize + 2,\n",
    "                stride=convstr,\n",
    "                padding=convpadding + 1\n",
    "            ),\n",
    "            nn.BatchNorm1d(planes//4))\n",
    "        self.conv1_3 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                inplanes,\n",
    "                planes//4,\n",
    "                kernel_size=1,\n",
    "                stride=convstr,\n",
    "                padding=0,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm1d(planes//4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(\n",
    "                in_channels=planes//4,\n",
    "                out_channels=planes//4,\n",
    "                kernel_size=convsize + 4,\n",
    "                stride=convstr,\n",
    "                padding=convpadding + 2\n",
    "            ),\n",
    "            nn.BatchNorm1d(planes//4))\n",
    "        self.conv1_4 = nn.Sequential(\n",
    "            nn.Conv1d(inplanes, planes//4, kernel_size=1, stride = convstr, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(planes//4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(\n",
    "                in_channels=planes//4,\n",
    "                out_channels=planes//4,\n",
    "                kernel_size=convsize + 6,\n",
    "                stride=convstr,\n",
    "                padding=convpadding+3\n",
    "            ),\n",
    "            nn.BatchNorm1d(planes//4))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.Inputconv1x1(x)\n",
    "\n",
    "        c1 = self.conv1_1(x)\n",
    "        c2 = self.conv1_2(x)\n",
    "        c3 = self.conv1_3(x)\n",
    "        c4 = self.conv1_4(x)\n",
    "\n",
    "        out = torch.cat([c1, c2, c3, c4], 1)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class IncUNet (nn.Module):\n",
    "    def __init__(self, in_shape):\n",
    "        super(IncUNet, self).__init__()\n",
    "        in_channels, height, width = in_shape\n",
    "        self.e1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            IncResBlock(64, 64))\n",
    "        self.e2 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            IncResBlock(128, 128))\n",
    "        self.e2add = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(128))\n",
    "        self.e3 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2,),\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            IncResBlock(256, 256))\n",
    "        self.e4 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(256, 256, kernel_size=4, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            IncResBlock(512, 512))\n",
    "        self.e4add = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(512))\n",
    "        self.e5 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            IncResBlock(512, 512))\n",
    "\n",
    "        self.e6 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            IncResBlock(512, 512))\n",
    "\n",
    "        self.e6add = nn.Sequential(\n",
    "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(512))\n",
    "\n",
    "        self.e7 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            IncResBlock(512, 512))\n",
    "\n",
    "        self.e8 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 512, kernel_size=4, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512))\n",
    "\n",
    "        self.d1 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 512, kernel_size=4, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            IncResBlock(512, 512))\n",
    "\n",
    "        self.d2 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            IncResBlock(512, 512))\n",
    "\n",
    "        self.d3 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(1024, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(p=0.5),\n",
    "            IncResBlock(512, 512))\n",
    "\n",
    "        self.d4 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            IncResBlock(512, 512))\n",
    "\n",
    "        self.d5 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            IncResBlock(512, 512))\n",
    "\n",
    "        self.d6 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(1024, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            IncResBlock(512, 512))\n",
    "\n",
    "        self.d7 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(1024, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(256, 256, kernel_size=4, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            IncResBlock(256, 256))\n",
    "\n",
    "        self.d8 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(512, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(128))\n",
    "\n",
    "        self.d9 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(128))\n",
    "\n",
    "        self.d10 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(256, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64))\n",
    "\n",
    "        self.out_l = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(128, in_channels, kernel_size=4, stride=2, padding=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        en1 = self.e1(x)\n",
    "        en2 = self.e2(en1)\n",
    "        en2add = self.e2add(en2)\n",
    "        en3 = self.e3(en2add)\n",
    "        en4 = self.e4(en3)\n",
    "        en4add = self.e4add(en4)\n",
    "        en5 = self.e5(en4add)\n",
    "        en6 = self.e6(en5)\n",
    "        en6add = self.e6add(en6)\n",
    "        en7 = self.e7(en6add)\n",
    "        en8 = self.e8(en7)\n",
    "\n",
    "        de1_ = self.d1(en8)\n",
    "        de1 = torch.cat([en7, de1_], 1)\n",
    "        de2_ = self.d2(de1)\n",
    "        de2 = torch.cat([en6add, de2_], 1)\n",
    "        de3_ = self.d3(de2)\n",
    "        de3 = torch.cat([en6, de3_], 1)\n",
    "        de4_ = self.d4(de3)\n",
    "        de4 = torch.cat([en5, de4_], 1)\n",
    "        de5_ = self.d5(de4)\n",
    "        de5 = torch.cat([en4add, de5_], 1)\n",
    "        de6_ = self.d6(de5)\n",
    "        de6 = torch.cat([en4, de6_], 1)\n",
    "        de7_ = self.d7(de6)\n",
    "        de7 = torch.cat([en3, de7_], 1)\n",
    "        de8_ = self.d8(de7)\n",
    "        de8 = torch.cat([en2add, de8_], 1)\n",
    "        de9_ = self.d9(de8)\n",
    "        de9 = torch.cat([en2, de9_], 1)\n",
    "        de10_ = self.d10(de9)\n",
    "        de10 = torch.cat([en1, de10_], 1)\n",
    "        out = self.out_l(de10)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(X):\n",
    "    return (X - np.min(X)) / (np.max(X) - np.min(X))\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    SIGNAL_PATH_COLUMN = 'PathToSignal'\n",
    "    TARGET_PATH_COLUMN = 'PathToTarget'\n",
    "    DATASET_COLUMN = 'Dataset'\n",
    "    CLASS_COLUMN = 'Disease'\n",
    "    LENGTH_COLUMN = 'Length'\n",
    "\n",
    "    def __init__(self, root_dir, df,\n",
    "                 transform=None,\n",
    "                 augmentation=None,\n",
    "                 config=None,\n",
    "                 softmax=False,\n",
    "                 fs=250):\n",
    "        \"\"\"\n",
    "        :param root_dir:\n",
    "        :param df:\n",
    "        :param transform:\n",
    "        :param augmentation:\n",
    "        :param config:\n",
    "        \"\"\"\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.df[self.SIGNAL_PATH_COLUMN] = self.df[self.SIGNAL_PATH_COLUMN].apply(str)\n",
    "        self.df[self.TARGET_PATH_COLUMN] = self.df[self.TARGET_PATH_COLUMN].apply(str)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.config = config\n",
    "        self.softmax = softmax\n",
    "        self.fs = fs\n",
    "\n",
    "        self.signal_cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __load_signal(self, path, start=0, end=-1, lead=None):\n",
    "        if path not in self.signal_cache.keys():\n",
    "            if path.endswith('.mat'):\n",
    "                x = loadmat(path)['ecg'].ravel().ravel()\n",
    "            else:\n",
    "                record = wfdb.rdrecord(path)\n",
    "                record_fs = record.fs\n",
    "\n",
    "                if lead is None or lead not in record.sig_name:\n",
    "                    x = record.p_signal[:, 0]\n",
    "                else:\n",
    "                    lead_idx = np.where(np.array(record.sig_name) == lead)[0][0]\n",
    "                    x = record.p_signal[:, lead_idx].ravel()\n",
    "                x = resample(x, num=int(x.size * self.fs / record_fs))\n",
    "            x_norm = min_max_normalize(x)\n",
    "            self.signal_cache[path] = x_norm\n",
    "\n",
    "        return self.signal_cache[path][start:end]\n",
    "\n",
    "    def __load_target(self, path, start, end):\n",
    "        target = np.load(path)['target'][start:end].ravel()\n",
    "\n",
    "        if self.softmax:\n",
    "            target = np.array([\n",
    "                target,\n",
    "                1. - target\n",
    "            ])\n",
    "        return target\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        info = self.df.iloc[idx]\n",
    "        start = info.Start\n",
    "        end = info.End\n",
    "        if 'Lead' in info.index:\n",
    "            lead = info.Lead\n",
    "        else:\n",
    "            lead = None\n",
    "\n",
    "        path_to_signal = os.path.join(\n",
    "            self.root_dir,\n",
    "            info[self.DATASET_COLUMN],\n",
    "            info[self.SIGNAL_PATH_COLUMN]\n",
    "        )\n",
    "\n",
    "        path_to_target = os.path.join(\n",
    "            self.root_dir,\n",
    "            info[self.DATASET_COLUMN],\n",
    "            info[self.TARGET_PATH_COLUMN]\n",
    "        )\n",
    "\n",
    "        loaded_signal = self.__load_signal(path_to_signal, lead=lead, start=start, end=end)\n",
    "        loaded_target = self.__load_target(path_to_target, start, end)\n",
    "\n",
    "        x = torch.tensor(\n",
    "            loaded_signal, dtype=torch.float32\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        y = torch.tensor(\n",
    "            loaded_target, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "import numpy as np\n",
    "import biosppy\n",
    "\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "# def peaks_finder(Z, threshold=0.6):\n",
    "#     Z[:20] = 0\n",
    "#     Z[-20:] = 0\n",
    "\n",
    "#     return find_peaks(Z, distance=50, height=0.3)[0]#find_peaks(Z, distance=220)[0]\n",
    "\n",
    "def peaks_finder(Z, threshold=0.6):\n",
    "    all_peaks_found = False\n",
    "\n",
    "    X = Z.copy()\n",
    "    peaks = []\n",
    "\n",
    "    while not all_peaks_found:\n",
    "        peak = np.argmax(X)\n",
    "        if X[peak] > threshold:\n",
    "            peaks.append(peak)\n",
    "            trim_from = max(0, peak - 200)\n",
    "            trim_to = min(len(X), peak + 200)\n",
    "            X[trim_from:trim_to] = 0\n",
    "        else:\n",
    "            all_peaks_found = True\n",
    "\n",
    "    return sorted(peaks)\n",
    "\n",
    "def normalize(X):\n",
    "    return (X - X.min()) / (X.max() - X.min())\n",
    "\n",
    "def filter_ecg(ecg, sampling_rate=250, filter_type=\"FIR\", filter_band=\"bandpass\",\n",
    "               filter_frequency=[2, 115], filter_order=2):\n",
    "    if filter_type in [\"FIR\", \"butter\", \"cheby1\", \"cheby2\", \"ellip\", \"bessel\"]:\n",
    "        order = int(filter_order * sampling_rate)\n",
    "        filtered, _, _ = biosppy.tools.filter_signal(signal=ecg,\n",
    "                                                     ftype=filter_type,\n",
    "                                                     band=filter_band,\n",
    "                                                     order=order,\n",
    "                                                     frequency=filter_frequency,\n",
    "                                                     sampling_rate=sampling_rate)\n",
    "    else:\n",
    "        raise ValueError('You can use only \"FIR\", \"butter\", \"cheby1\", \"cheby2\", \"ellip\", \"bessel\" filter types')\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_segmentation_model(config):\n",
    "    if config['seg_model_name'].lower() == 'rpnet':\n",
    "        seg_model = IncUNet(in_shape=config['seg_inp_shape'])\n",
    "    else:\n",
    "        seg_model = SignalNet(n_channels=config['seg_inp_shape'], n_class=config['seg_num_classes'])\n",
    "        \n",
    "    if config['seg_path_to_weights']:\n",
    "        seg_model.load_state_dict(torch.load(config['seg_path_to_weights'])['state_dict'])\n",
    "    seg_model.eval()\n",
    "    return seg_model\n",
    "\n",
    "def create_classifier(config):\n",
    "    if config['classifier_model_name'].lower() == 'resnet':\n",
    "        model = ResNet()\n",
    "    elif config['classifier_model_name'].lower() == 'inceptiontime':\n",
    "        model = InceptionTime()\n",
    "    else:\n",
    "        model = RNNModel()\n",
    "    if config['classifier_path_to_weights']:\n",
    "        model = model.load_from_checkpoint(config['classifier_path_to_weights'])\n",
    "    \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "class PVCDetector(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seg_threshold = config['seg_threshold']\n",
    "        self.seg_model = create_segmentation_model(config)\n",
    "        self.classifier = create_classifier(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        seg_out = self.seg_model(x)\n",
    "        \n",
    "        out = seg_out.cpu().detach().squeeze(0).numpy()\n",
    "        peaks = np.array([peaks_finder(out[i], threshold=self.seg_threshold) for i in range(out.shape[0])])[0]\n",
    "        left_pad = 200 - peaks[0] if peaks[0] - 200 < 0 else 0\n",
    "        right_pad = (peaks[-1] + 200+left_pad) - x.shape[2] if peaks[-1] + 200 > x.shape[2] else 0\n",
    "        padded_x = pad(x, (left_pad, right_pad), mode='constant', value=0)\n",
    "        \n",
    "        output = []\n",
    "        for peak in peaks:\n",
    "            peak_ind_with_pad = peak + left_pad\n",
    "            cur_heart_beat = padded_x[:, :, peak_ind_with_pad-150:peak_ind_with_pad+150]\n",
    "            classifier_out = self.classifier(cur_heart_beat)\n",
    "            pvc_class = classifier_out.argmax().item()\n",
    "            output.append((peak, pvc_class))\n",
    "        return output, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # SEGMENTATION\n",
    "    'seg_model_name': 'unet', # rpnet or unet\n",
    "    'seg_inp_shape': 1, #(1, 1, 5000),\n",
    "    'seg_num_classes': 1,\n",
    "    'seg_threshold': 0.6,\n",
    "    \n",
    "#     'seg_path_to_weights': '/home/ikachko/Galene_2/checkpoints/rpnet_2_classes_5000_smooth_l1/rpnet_2_classes_5000_smooth_l1.pt', \n",
    "#     'seg_path_to_weights': '/home/ikachko/Galene_2/checkpoints/unet_2_classes_5000_smooth_l1/unet_2_classes_5000_smooth_l1.pt',\n",
    "    'seg_path_to_weights': '/home/ikachko/Galene_2/checkpoints/mitbih_unet_5000_smooth_l1/by_f1_0.98mitbih_unet_5000_smooth_l1.pt',\n",
    "    # CLASSIFICATION\n",
    "    'classifier_model_name': 'ResNet', # LSTM or IncTime or ResNet\n",
    "    'classifier_inp_shape': 1,\n",
    "    'classifier_num_classes': 1,\n",
    "#     'classifier_path_to_weights': '/home/bohdan/PeakDetector/mit_svdb_exps/version_29/checkpoints/epoch=232.ckpt'\n",
    "    'classifier_path_to_weights': '/home/bohdan/PeakDetector/logs/raw_input_mit_bih_lstm/version_22/checkpoints/epoch=3.ckpt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from biosppy.signals.ecg import correct_rpeaks\n",
    "import math\n",
    "\n",
    "class ChinaDataset(Dataset):\n",
    "    SIGNAL_PATH_COLUMN = 'PathToData'\n",
    "    CLASS_COLUMN = 'Label'\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        :param root_dir:\n",
    "        :param df:\n",
    "        :param transform:\n",
    "        :param augmentation:\n",
    "        :param config:\n",
    "        \"\"\"\n",
    "        random.seed(42)\n",
    "        self.root_dir = root_dir\n",
    "        self.resample_ratio = 250 / 400\n",
    "        self.patients = ['A02.mat', 'A03.mat', 'A04.mat', 'A07.mat', 'A08.mat', 'A10.mat']\n",
    "        self.signal_cache_x = {}\n",
    "        self.signal_cache_y = {}\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.patients)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        path_to_file = self.patients[idx]\n",
    "\n",
    "\n",
    "        if path_to_file not in self.signal_cache_x.keys():\n",
    "            cur_pat = np.expand_dims(np.squeeze(np.asarray(\n",
    "                loadmat(os.path.join(self.root_dir, path_to_file))['ecg'], dtype=np.float64), 1), 1)\n",
    "            cur_pat = normalize(cur_pat)\n",
    "            self.signal_cache_x[path_to_file] = resample(cur_pat, num=int(cur_pat.size * 250 / 400))\n",
    "            \n",
    "            self.signal_cache_y[path_to_file] = np.squeeze(loadmat(os.path.join(self.root_dir, path_to_file.replace('A', 'R')))['ref'][0][0][1], 1)\n",
    "            self.signal_cache_y[path_to_file] = (self.signal_cache_y[path_to_file] * self.resample_ratio).astype(int)\n",
    "            self.signal_cache_y[path_to_file] = correct_rpeaks(self.signal_cache_x[path_to_file], self.signal_cache_y[path_to_file], sampling_rate=250)['rpeaks']\n",
    "\n",
    "        x = self.signal_cache_x[path_to_file]\n",
    "        x = np.expand_dims(np.squeeze(x, 1), 0)\n",
    "        x = torch.tensor(x.copy(), dtype=torch.float64)\n",
    "\n",
    "        y = self.signal_cache_y[path_to_file]\n",
    "\n",
    "        return x.float(), y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_p = '/datasets/extra_space2/ikachko/ecg/china_challenge/TrainingSet'\n",
    "d = ChinaDataset(file_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = '/datasets/extra_space2/ikachko/ecg/svdb_conv_25_5000.csv'\n",
    "root_dir = '/datasets/extra_space2/ikachko/ecg/'\n",
    "df = pd.read_csv(path_to_csv)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = SegmentationDataset(\n",
    "    root_dir=root_dir,\n",
    "    df=df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "window_size = 5000\n",
    "start = 800\n",
    "step = 400\n",
    "\n",
    "detector = PVCDetector(config)\n",
    "sigmoid = nn.Sigmoid()\n",
    "for i in tqdm.tqdm(range(len(d))):\n",
    "#     x, y = dataset[i]\n",
    "    x, y = d[i]\n",
    "    \n",
    "    cur_peaks = []\n",
    "    for i in range(math.ceil(len(x[0]) / step)):\n",
    "        cur_x = x[:, start:start+window_size]\n",
    "        if len(cur_x[0]) != window_size:\n",
    "            cur_x = pad(cur_x, (0, window_size - len(cur_x[0])), mode='constant', value=0)\n",
    "                \n",
    "        preds, raw_out = detector(cur_x.unsqueeze(0))\n",
    "        \n",
    "#         preds = [(x[0]+start, x[1]) for x in preds]\n",
    "        \n",
    "        cur_x = cur_x[0].detach()\n",
    "        pvc_peaks = [x[0] for x in preds if x[1]]\n",
    "        normal_peaks = [x[0] for x in preds if not x[1]]\n",
    "        \n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(cur_x)\n",
    "        plt.plot(raw_out[0])\n",
    "        plt.scatter(pvc_peaks, cur_x[pvc_peaks], color='red', s=100)\n",
    "        plt.scatter(normal_peaks, cur_x[normal_peaks], color='green', s=100)\n",
    "        start += step\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thrash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biosppy.signals.ecg import correct_rpeaks\n",
    "from scipy import signal\n",
    "import os\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm \n",
    "import gc\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "from scipy.signal import resample\n",
    "import sys\n",
    "from seaborn import scatterplot\n",
    "import random \n",
    "from biosppy.signals.ecg import correct_rpeaks\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('/home/bohdan/PeakDetector')\n",
    "\n",
    "from modules.baseline import ResNet\n",
    "# from local_models import SignalNet\n",
    "# from utils import normalize, peaks_finder, filter_ecg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = '/datasets/extra_space2/ikachko/ecg/PVC_mit_bih.csv'\n",
    "d = pd.read_csv(df_name)\n",
    "exp_path = '/datasets/extra_space2/ikachko/ecg/'\n",
    "\n",
    "x_or = np.asarray(wfdb.rdrecord(os.path.join(exp_path, d['Dataset'].iloc[0], str(d['PathToData'].iloc[0]))).p_signal[:, 0], dtype=np.float64)\n",
    "x = signal.resample(x, num=int(x.size * 250 / 360))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = correct_rpeaks(x, d[d['PathToData']==100]['Location'], sampling_rate=250)['rpeaks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(x[:10000])\n",
    "merged = a #d[d['PathToData']==100]['Location']\n",
    "fif = [x for x in merged if x < 10000 ]\n",
    "plt.scatter(fif, x[:10000][fif], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.read_csv('/home/ikachko/Galene_2/classic_metrics_1.csv'),pd.read_csv('/home/ikachko/Galene_2/classic_metrics_5.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_of_correct_peaks(peaks_pred, peaks_true, eps=25):\n",
    "    \"\"\"\n",
    "    TP -- number of predicted peaks, that are close to true peaks\n",
    "        If dist(peaks_pred, peaks_true[i]) < eps:\n",
    "            TP += 1\n",
    "    FP -- number of peaks, that I found, but it was not true\n",
    "        len(peaks_pred) - TP\n",
    "    FN -- number of peaks, that I did not found\n",
    "        len(peak_true) - TP\n",
    "    :param peaks_pred:\n",
    "    :param peaks_true:\n",
    "    :param eps:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    N_all = len(peaks_true)\n",
    "\n",
    "    TP = 0\n",
    "    if len(peaks_pred) == 0 or len(peaks_true) == 0:\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = len(peaks_true) - TP\n",
    "        N_all = len(peaks_true)\n",
    "        return TP, FP, FN, N_all\n",
    "    \n",
    "    distances = np.min(np.array([abs(peaks_true - peaks_pred[i]) for i in range(len(peaks_pred))]), 0)\n",
    "\n",
    "    TP = (distances < eps).astype(int).sum()\n",
    "    FP = len(peaks_pred) - TP\n",
    "    FN = len(peaks_true) - TP\n",
    "\n",
    "    return TP, FP, FN, N_all\n",
    "\n",
    "import random \n",
    "from biosppy.signals.ecg import correct_rpeaks\n",
    "import math\n",
    "\n",
    "class ChinaDataset(Dataset):\n",
    "    SIGNAL_PATH_COLUMN = 'PathToData'\n",
    "    CLASS_COLUMN = 'Label'\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        :param root_dir:\n",
    "        :param df:\n",
    "        :param transform:\n",
    "        :param augmentation:\n",
    "        :param config:\n",
    "        \"\"\"\n",
    "        random.seed(42)\n",
    "        self.root_dir = root_dir\n",
    "        self.resample_ratio = 250 / 400\n",
    "        self.patients = ['A02.mat', 'A03.mat', 'A04.mat', 'A07.mat', 'A08.mat', 'A10.mat']\n",
    "        self.signal_cache_x = {}\n",
    "        self.signal_cache_y = {}\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.patients)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        path_to_file = self.patients[idx]\n",
    "\n",
    "\n",
    "        if path_to_file not in self.signal_cache_x.keys():\n",
    "            cur_pat = np.expand_dims(np.squeeze(np.asarray(\n",
    "                loadmat(os.path.join(self.root_dir, path_to_file))['ecg'], dtype=np.float64), 1), 1)\n",
    "#             cur_pat = normalize(cur_pat)\n",
    "            self.signal_cache_x[path_to_file] = resample(cur_pat, num=int(cur_pat.size * 250 / 400))\n",
    "            \n",
    "            self.signal_cache_y[path_to_file] = np.squeeze(loadmat(os.path.join(self.root_dir, path_to_file.replace('A', 'R')))['ref'][0][0][1], 1)\n",
    "            self.signal_cache_y[path_to_file] = (self.signal_cache_y[path_to_file] * self.resample_ratio).astype(int)\n",
    "            self.signal_cache_y[path_to_file] = correct_rpeaks(self.signal_cache_x[path_to_file], self.signal_cache_y[path_to_file], sampling_rate=250)['rpeaks']\n",
    "\n",
    "        x = self.signal_cache_x[path_to_file]\n",
    "        x = np.expand_dims(np.squeeze(x, 1), 0)\n",
    "        x = torch.tensor(x.copy(), dtype=torch.float64)\n",
    "\n",
    "        y = self.signal_cache_y[path_to_file]\n",
    "\n",
    "        return x.float(), y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_p = '/datasets/extra_space2/ikachko/ecg/china_challenge/TrainingSet'\n",
    "d = ChinaDataset(file_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(d))):\n",
    "    if d.patients[i] == 'A04.mat':\n",
    "        continue\n",
    "    x, y = d[i]\n",
    "    pred = pd.read_csv('/home/ikachko/Galene_2/classic_peaks_{}.csv'.format(str(i)))\n",
    "    pred = pred[pred['Algorithm'] == 'Hamilton']['Location']\n",
    "    iTP, iFP, iFN, N_a = get_num_of_correct_peaks(np.array(pred), y)\n",
    "    TP = iTP\n",
    "    FP = iFP\n",
    "    FN = iFN\n",
    "    N_true = N_a\n",
    "    \n",
    "    acc = TP / len(y)\n",
    "    print(round(acc, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(pd.read_csv('/home/ikachko/Galene_2/classic_peaks_{}.csv'.format(str(0)))['Algorithm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(x[0][:10000])\n",
    "fif = [x for x in pred if x < 10000]\n",
    "kok = [x for x in y if x < 10000]\n",
    "plt.scatter(kok, x[0][:10000][kok], color='green',linewidths=10)\n",
    "plt.scatter(fif, x[0][:10000][fif], color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/datasets/extra_space2/ikachko/ecg/mitdb_svdb_resampled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_or = np.asarray(wfdb.rdrecord(os.path.join(exp_path, d['Dataset'].iloc[0], str(d['PathToData'].iloc[0]))).p_signal[:, 0], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_or[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
